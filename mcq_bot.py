import logging
import os
import asyncio
from telegram import Update, Document, InputFile, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import (
    Application,
    CommandHandler,
    MessageHandler,
    filters,
    ContextTypes,
    CallbackQueryHandler # <-- Ø¥Ø¶Ø§ÙØ© CallbackQueryHandler
)
from PyPDF2 import PdfReader
import google.generativeai as genai
import time

# --- Ø£Ø¯Ø®Ù„ Ø§Ù„ØªÙˆÙƒÙ†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ù‡Ù†Ø§ Ù…Ø¨Ø§Ø´Ø±Ø© ---
TELEGRAM_BOT_TOKEN = "6608888663:AAGMZrD-c328tqXCZYEkKBjGUfCsqmPlJrk"  # <-- Ø¶Ø¹ ØªÙˆÙƒÙ† ØªÙ„ÙŠØ¬Ø±Ø§Ù… Ù‡Ù†Ø§
GOOGLE_API_KEY = "AIzaSyAB24hOiaVwfOjDl36RbpMetlBqW1a7jDs"      # <-- Ø¶Ø¹ Ù…ÙØªØ§Ø­ Google API Ù‡Ù†Ø§
# -----------------------------------------

# --- Ø¥Ø¹Ø¯Ø§Ø¯ Logging ---
logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s", level=logging.INFO
)
logger = logging.getLogger(__name__)
# ---------------------------

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„ØªÙˆÙƒÙ†Ø§Øª
if not TELEGRAM_BOT_TOKEN or TELEGRAM_BOT_TOKEN == "YOUR_ACTUAL_TELEGRAM_BOT_TOKEN":
    print("ERROR: Please replace 'YOUR_ACTUAL_TELEGRAM_BOT_TOKEN' with your actual Telegram Bot Token in the code.")
    raise ValueError("Telegram Bot Token not set correctly.")
if not GOOGLE_API_KEY or GOOGLE_API_KEY == "YOUR_ACTUAL_GOOGLE_API_KEY":
     print("ERROR: Please replace 'YOUR_ACTUAL_GOOGLE_API_KEY' with your actual Google API Key in the code.")
     raise ValueError("Google API Key not set correctly.")


# Ø¥Ø¹Ø¯Ø§Ø¯ Google Generative AI SDK
try:
    genai.configure(api_key=GOOGLE_API_KEY)
    logger.info("Google Generative AI SDK configured successfully.")
    GEMINI_MODEL = 'gemini-1.5-flash-latest'
    model = genai.GenerativeModel(GEMINI_MODEL)
    logger.info(f"Using Gemini model: {GEMINI_MODEL}")
except Exception as e:
    logger.error(f"Failed to configure Google Generative AI SDK: {e}")
    model = None

# --- ØªØ¹Ø±ÙŠÙ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø²Ø±Ø§Ø± (Callback Data) ---
CALLBACK_GENERATE_5 = "generate_5"
CALLBACK_GENERATE_10 = "generate_10"
CALLBACK_GENERATE_SPECIFY = "generate_specify"
CALLBACK_CLEAR = "clear_pdfs"
CALLBACK_UPLOAD_INFO = "upload_info" # Ø²Ø± Ù„Ø¥Ø¸Ù‡Ø§Ø± Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø±ÙØ¹

# --- Ø¯Ø§Ù„Ø§Øª Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ ---

async def _perform_clear(chat_id: int, context: ContextTypes.DEFAULT_TYPE) -> str:
    """Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù…Ø³Ø­ Ù…Ù„ÙØ§Øª PDF."""
    if chat_id in context.user_data and 'pdfs' in context.user_data[chat_id] and context.user_data[chat_id]['pdfs']:
        count = len(context.user_data[chat_id]['pdfs'])
        context.user_data[chat_id]['pdfs'] = []
        logger.info(f"Cleared {count} PDFs for chat_id {chat_id}")
        return f"ğŸ—‘ï¸ ØªÙ… Ù…Ø³Ø­ {count} Ù…Ø­Ø§Ø¶Ø±Ø©/Ù…Ø­Ø§Ø¶Ø±Ø§Øª Ù…Ø±ÙÙˆØ¹Ø© Ø¨Ù†Ø¬Ø§Ø­."
    else:
        logger.info(f"No PDFs to clear for chat_id {chat_id}")
        return "âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø­Ø§Ø¶Ø±Ø§Øª Ù…Ø±ÙÙˆØ¹Ø© Ù„Ù…Ø³Ø­Ù‡Ø§."

async def _perform_generation(update: Update, context: ContextTypes.DEFAULT_TYPE, num_questions_args: list[str]) -> None:
    """Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©."""
    chat_id = update.effective_chat.id
    message = update.effective_message # Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø£Ù…Ø± Ø£Ùˆ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø²Ø±

    if not model:
         await message.reply_text("Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù‡Ù†Ø§Ùƒ Ù…Ø´ÙƒÙ„Ø© ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Google AI. Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø­Ø§Ù„ÙŠØ§Ù‹.")
         return

    if chat_id not in context.user_data or not context.user_data[chat_id].get('pdfs'):
        await message.reply_text("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø±ÙØ¹ Ø£ÙŠ Ù…Ù„ÙØ§Øª PDF. ÙŠØ±Ø¬Ù‰ Ø±ÙØ¹ Ù…Ø­Ø§Ø¶Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ø£ÙˆÙ„Ø§Ù‹.")
        return

    num_pdfs_uploaded = len(context.user_data[chat_id]['pdfs'])
    num_questions_per_pdf = []

    try:
        if len(num_questions_args) == 1:
            num_q = int(num_questions_args[0])
            if num_q <= 0 or num_q > 20:
                await message.reply_text("âš ï¸ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø¨ÙŠÙ† 1 Ùˆ 20 Ù„ÙƒÙ„ Ù…Ø­Ø§Ø¶Ø±Ø©.")
                return
            num_questions_per_pdf = [num_q] * num_pdfs_uploaded
        elif len(num_questions_args) == num_pdfs_uploaded:
            num_questions_per_pdf = [int(arg) for arg in num_questions_args]
            if any(n <= 0 or n > 20 for n in num_questions_per_pdf):
                await message.reply_text("âš ï¸ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ù„ÙƒÙ„ Ù…Ø­Ø§Ø¶Ø±Ø© ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø¨ÙŠÙ† 1 Ùˆ 20.")
                return
        else:
            await message.reply_text(
                f"Ù„Ù‚Ø¯ Ø£Ø±Ø³Ù„Øª {num_pdfs_uploaded} Ù…Ø­Ø§Ø¶Ø±Ø©/Ù…Ø­Ø§Ø¶Ø±Ø§Øª.\n"
                f"ØªØ­ØªØ§Ø¬ Ù„ØªØ­Ø¯ÙŠØ¯ Ø¥Ù…Ø§ Ø¹Ø¯Ø¯Ù‹Ø§ ÙˆØ§Ø­Ø¯Ù‹Ø§ Ù…Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© (ÙŠØ·Ø¨Ù‚ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„) Ø£Ùˆ {num_pdfs_uploaded} Ø£Ø¹Ø¯Ø§Ø¯ Ù…Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ù…Ø±:\n"
                f"`/generate N1 N2 ...`" # ØªÙˆØ¬ÙŠÙ‡ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ù…Ø± Ø§Ù„Ù†ØµÙŠ Ù„Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø®ØµØµ
                 "\n\nØ£Ùˆ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø®ØªÙŠØ§Ø± Ø£Ø­Ø¯ Ø£Ø²Ø±Ø§Ø± Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø³Ø±ÙŠØ¹ (Ù…Ø«Ù„ 5 Ø£Ùˆ 10 Ù„Ù„ÙƒÙ„).",
                parse_mode='Markdown'
            )
            return
    except ValueError:
        await message.reply_text("âš ï¸ Ù…Ù† ÙØ¶Ù„Ùƒ Ø£Ø¯Ø®Ù„ Ø£Ø±Ù‚Ø§Ù…Ù‹Ø§ ØµØ­ÙŠØ­Ø© Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©.")
        return

    await message.reply_text(f"â³ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ù„Ù€ {num_pdfs_uploaded} Ù…Ø­Ø§Ø¶Ø±Ø©/Ù…Ø­Ø§Ø¶Ø±Ø§Øª...")

    all_results = []
    generation_errors = 0

    for i, pdf_data in enumerate(context.user_data[chat_id]['pdfs']):
        lecture_name = pdf_data['filename']
        lecture_text = pdf_data['text']
        num_q_for_this_lecture = num_questions_per_pdf[i]

        if "Error:" in lecture_text or not lecture_text.strip():
            error_msg = lecture_text if "Error:" in lecture_text else "Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ ÙØ§Ø±Øº."
            result = (f"---  LECTURE: {lecture_name} ---\n"
                     f"âŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø³Ø¦Ù„Ø© Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø­Ø§Ø¶Ø±Ø©: {error_msg}")
            all_results.append(result)
            logger.warning(f"Skipping MCQ generation for {lecture_name} due to text extraction issue: {error_msg}")
            generation_errors += 1
            continue

        # Ø¥Ø¸Ù‡Ø§Ø± Ø¥Ø´Ø¹Ø§Ø± Ø¨Ø§Ù„ÙƒØªØ§Ø¨Ø© ÙˆØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        await context.bot.send_chat_action(chat_id=chat_id, action='typing')
        status_message_text = f"({i+1}/{num_pdfs_uploaded}) âš™ï¸ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ {num_q_for_this_lecture} Ø³Ø¤Ø§Ù„/Ø£Ø³Ø¦Ù„Ø© Ù„Ù€: {lecture_name}..."
        # Ù„Ø§ Ù†Ø±Ø³Ù„ Ø±Ø³Ø§Ù„Ø© Ø­Ø§Ù„Ø© Ù„ÙƒÙ„ Ù…Ù„Ù Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø¥Ø²Ø¹Ø§Ø¬ØŒ Ù†Ø¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ©
        logger.info(status_message_text)


        mcqs_text = await generate_mcqs_from_text(lecture_text, num_q_for_this_lecture, lecture_name)

        response_header = f"--- âœ… LECTURE: {lecture_name} ---\n\n"
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø®Ø·Ø£ Ù…Ù† Ø§Ù„Ø¯Ø§Ù„Ø© Ù†ÙØ³Ù‡Ø§
        if "Sorry, I couldn't generate MCQs" in mcqs_text or "blocked by safety filters" in mcqs_text:
             response_header = f"--- âš ï¸ LECTURE: {lecture_name} ---\n\n"
             generation_errors += 1


        full_response = response_header + mcqs_text
        all_results.append(full_response)

        # ØªØ£Ø®ÙŠØ± Ø¨Ø³ÙŠØ·
        await asyncio.sleep(1.5)

    # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø¬Ù…Ø¹Ø©
    await message.reply_text("--- ğŸ“ Ù†ØªØ§Ø¦Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ---")
    combined_message = ""
    for result in all_results:
        if len(combined_message) + len(result) + 2 > 4096: # +2 for potential newlines
            try:
                await message.reply_text(combined_message)
            except Exception as send_error:
                logger.error(f"Error sending combined message part: {send_error}")
                await message.reply_text("Ø®Ø·Ø£ ÙÙŠ Ø¥Ø±Ø³Ø§Ù„ Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬.")
            combined_message = result + "\n\n"
        else:
            combined_message += result + "\n\n"

    if combined_message:
        try:
            await message.reply_text(combined_message.strip())
        except Exception as send_error:
             logger.error(f"Error sending final combined message part: {send_error}")
             await message.reply_text("Ø®Ø·Ø£ ÙÙŠ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£Ø®ÙŠØ± Ù…Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬.")

    final_message = "ğŸ ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©!"
    if generation_errors > 0:
        final_message += f"\n\nÙ„Ø§Ø­Ø¸: ÙˆØ§Ø¬Ù‡Øª Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ù„Ù€ {generation_errors} Ù…Ø­Ø§Ø¶Ø±Ø©/Ù…Ø­Ø§Ø¶Ø±Ø§Øª (ÙŠØ±Ø¬Ù‰ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø£Ø¹Ù„Ø§Ù‡)."

    final_message += "\n\n**ÙŠØ±Ø¬Ù‰ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¯Ù‚ØªÙ‡Ø§ ÙˆÙ…Ù„Ø§Ø¡Ù…ØªÙ‡Ø§.**"
    final_message += "\n\nÙŠÙ…ÙƒÙ†Ùƒ Ø±ÙØ¹ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø­Ø§Ø¶Ø±Ø§Øª Ø£Ùˆ Ù…Ø³Ø­ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ§Ù„Ø¨Ø¯Ø¡ Ù…Ù† Ø¬Ø¯ÙŠØ¯."
    await message.reply_text(final_message)
    # Ù„Ø§ Ù†Ù…Ø³Ø­ Ø§Ù„Ù…Ù„ÙØ§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¨Ø¹Ø¯ Ø§Ù„Ø¢Ù†ØŒ Ù„ÙŠØ¨Ù‚Ù‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù…ØªØ­ÙƒÙ…Ù‹Ø§ Ø¹Ø¨Ø± Ø²Ø± Ø§Ù„Ù…Ø³Ø­


# --- Ø¯Ø§Ù„Ø© Ø¥Ù†Ø´Ø§Ø¡ Ù„ÙˆØ­Ø© Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ---
def build_main_keyboard(context: ContextTypes.DEFAULT_TYPE, chat_id: int) -> InlineKeyboardMarkup:
    """ÙŠÙ†Ø´Ø¦ ÙˆÙŠØ¹ÙŠØ¯ Ù„ÙˆØ­Ø© Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©."""
    buttons = [
        [InlineKeyboardButton("ğŸ“„ Ø¥Ø±Ø³Ø§Ù„ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø§Ø¶Ø±Ø§Øª (PDF)", callback_data=CALLBACK_UPLOAD_INFO)],
        [
            InlineKeyboardButton("âš¡ Ø¥Ù†Ø´Ø§Ø¡ 5 Ø£Ø³Ø¦Ù„Ø©/Ù…Ø­Ø§Ø¶Ø±Ø©", callback_data=CALLBACK_GENERATE_5),
            InlineKeyboardButton("âš¡ Ø¥Ù†Ø´Ø§Ø¡ 10 Ø£Ø³Ø¦Ù„Ø©/Ù…Ø­Ø§Ø¶Ø±Ø©", callback_data=CALLBACK_GENERATE_10)
        ],
        [InlineKeyboardButton("âš™ï¸ ØªØ­Ø¯ÙŠØ¯ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© (Ø¹Ø¨Ø± Ø£Ù…Ø± Ù†ØµÙŠ)", callback_data=CALLBACK_GENERATE_SPECIFY)],
        [InlineKeyboardButton("ğŸ—‘ï¸ Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¶Ø±Ø§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©", callback_data=CALLBACK_CLEAR)]
    ]
    # Ø¹Ø±Ø¶ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø­Ø§Ø¶Ø±Ø§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø© Ø­Ø§Ù„ÙŠÙ‹Ø§ Ø¥Ù† ÙˆØ¬Ø¯
    pdf_count = 0
    if chat_id in context.user_data and 'pdfs' in context.user_data[chat_id]:
        pdf_count = len(context.user_data[chat_id]['pdfs'])

    # Ù„Ø§ Ù†Ø¶ÙŠÙ Ø§Ù„Ø²Ø± Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¹Ø¯Ø¯ ØµÙØ±Ù‹Ø§ - Ù„ØªØ¬Ù†Ø¨ ØªÙƒØ±Ø§Ø± Ø²Ø± Ø§Ù„Ù…Ø³Ø­
    # if pdf_count > 0:
    #     buttons.append([InlineKeyboardButton(f"ğŸ—‘ï¸ Ù…Ø³Ø­ ({pdf_count}) Ù…Ø­Ø§Ø¶Ø±Ø§Øª Ù…Ø±ÙÙˆØ¹Ø©", callback_data=CALLBACK_CLEAR)])
    # else:
        # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ø²Ø± Ø¢Ø®Ø± Ù‡Ù†Ø§ Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù‡Ù†Ø§Ùƒ Ù…Ù„ÙØ§Øª
        # pass

    return InlineKeyboardMarkup(buttons)

# --- Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø£ÙˆØ§Ù…Ø± ---
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """ÙŠØ±Ø³Ù„ Ø±Ø³Ø§Ù„Ø© ØªØ±Ø­ÙŠØ¨ ÙˆÙ„ÙˆØ­Ø© Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©."""
    user = update.effective_user
    chat_id = update.effective_chat.id
    logger.info(f"User {user.first_name} (ID: {user.id}) started the bot in chat {chat_id}")

    # ØªÙ‡ÙŠØ¦Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø©
    if chat_id not in context.user_data:
        context.user_data[chat_id] = {'pdfs': []}

    keyboard = build_main_keyboard(context, chat_id)
    await update.message.reply_html(
        rf"Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ ÙŠØ§ {user.mention_html()}!",
        reply_markup=keyboard
    )
    await update.message.reply_text(
         "Ø£Ù†Ø§ Ø¨ÙˆØª Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ù…ØªØ¹Ø¯Ø¯ (MCQs) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Google AI (Gemini).\n\n"
         "**ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:**\n"
         "1. Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ Ø²Ø± 'ğŸ“„ Ø¥Ø±Ø³Ø§Ù„ Ù…Ù„ÙØ§Øª...' Ù„Ù…Ø¹Ø±ÙØ© ÙƒÙŠÙÙŠØ© Ø§Ù„Ø±ÙØ¹.\n"
         "2. Ø£Ø±Ø³Ù„ Ù…Ù„Ù Ø£Ùˆ Ø£ÙƒØ«Ø± Ù…Ù† Ù…Ù„ÙØ§Øª PDF Ù„Ù„Ù…Ø­Ø§Ø¶Ø±Ø§Øª.\n"
         "3. Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„Ø±ÙØ¹ØŒ Ø§Ø®ØªØ± Ø£Ø­Ø¯ Ø£Ø²Ø±Ø§Ø± 'Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©' (5 Ø£Ùˆ 10 Ù„Ù„ÙƒÙ„).\n"
         "4. Ø£ÙˆØŒ Ø§Ø¶ØºØ· 'âš™ï¸ ØªØ­Ø¯ÙŠØ¯ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©' ÙˆØ§ØªØ¨Ø¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù„Ø¥Ø±Ø³Ø§Ù„ Ø£Ù…Ø± Ù†ØµÙŠ Ù…Ø«Ù„ `/generate 3 7` (3 Ù„Ù„Ø£ÙˆÙ„Ù‰ØŒ 7 Ù„Ù„Ø«Ø§Ù†ÙŠØ©).\n"
         "5. Ø§Ø³ØªØ®Ø¯Ù… Ø²Ø± 'ğŸ—‘ï¸ Ù…Ø³Ø­' Ù„Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¶Ø±Ø§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø© ÙˆØ§Ù„Ø¨Ø¯Ø¡ Ù…Ù† Ø¬Ø¯ÙŠØ¯."
    )


# --- Ù…Ø¹Ø§Ù„Ø¬ Ù†Ù‚Ø±Ø§Øª Ø§Ù„Ø£Ø²Ø±Ø§Ø± ---
async def button_callback(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """ÙŠØ¹Ø§Ù„Ø¬ Ù†Ù‚Ø±Ø§Øª Ø§Ù„Ø£Ø²Ø±Ø§Ø± Ø§Ù„Ù…Ø¶Ù…Ù†Ø©."""
    query = update.callback_query
    await query.answer() # Ù…Ù‡Ù… Ø¬Ø¯Ù‹Ø§ Ù„Ø¥ÙŠÙ‚Ø§Ù Ø¥Ø´Ø§Ø±Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø²Ø±

    callback_data = query.data
    chat_id = query.message.chat_id
    logger.info(f"Callback query received: {callback_data} from chat_id {chat_id}")

    if callback_data == CALLBACK_UPLOAD_INFO:
        await query.message.reply_text(
            "ğŸ“¤ ÙŠØ±Ø¬Ù‰ Ø¥Ø±Ø³Ø§Ù„ Ù…Ù„ÙØ§Øª PDF Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø³Ø¦Ù„Ø© Ù„Ù‡Ø§ Ù…Ø¨Ø§Ø´Ø±Ø© ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©. ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø±Ø³Ø§Ù„ Ø¹Ø¯Ø© Ù…Ù„ÙØ§Øª ÙˆØ§Ø­Ø¯Ù‹Ø§ ØªÙ„Ùˆ Ø§Ù„Ø¢Ø®Ø±."
        )
        # ÙŠÙ…ÙƒÙ† Ø¥Ø¹Ø§Ø¯Ø© Ø¥Ø±Ø³Ø§Ù„ Ù„ÙˆØ­Ø© Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø¥Ø°Ø§ Ø£Ø±Ø¯Ù†Ø§ Ø£Ù† ØªØ¨Ù‚Ù‰ Ø¸Ø§Ù‡Ø±Ø©
        # keyboard = build_main_keyboard(context, chat_id)
        # await query.edit_message_reply_markup(reply_markup=keyboard) # ØªØ­Ø±ÙŠØ± Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù„Ø¥Ø¨Ù‚Ø§Ø¡ Ø§Ù„Ø£Ø²Ø±Ø§Ø±

    elif callback_data == CALLBACK_CLEAR:
        clear_message = await _perform_clear(chat_id, context)
        # ØªØ­Ø±ÙŠØ± Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙˆØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£Ø²Ø±Ø§Ø±
        keyboard = build_main_keyboard(context, chat_id)
        try:
            await query.edit_message_text(
                text=f"{query.message.text}\n\n---\n{clear_message}", # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù„Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
                reply_markup=keyboard,
                parse_mode='HTML' # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© HTML
            )
        except Exception as e: # Ù‚Ø¯ ØªÙØ´Ù„ Ø¥Ø°Ø§ Ù„Ù… ØªØªØºÙŠØ± Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø£Ùˆ Ù„ÙˆØ­Ø© Ø§Ù„Ù…ÙØ§ØªÙŠØ­
            logger.warning(f"Failed to edit message after clear: {e}. Sending new message.")
            await query.message.reply_text(clear_message, reply_markup=keyboard) # Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø© ÙƒØ¨Ø¯ÙŠÙ„


    elif callback_data == CALLBACK_GENERATE_5:
        await _perform_generation(update, context, ['5'])

    elif callback_data == CALLBACK_GENERATE_10:
         await _perform_generation(update, context, ['10'])

    elif callback_data == CALLBACK_GENERATE_SPECIFY:
        await query.message.reply_text(
            "â„¹ï¸ Ù„ØªØ­Ø¯ÙŠØ¯ Ø¹Ø¯Ø¯ Ø£Ø³Ø¦Ù„Ø© Ù…Ø®ØªÙ„Ù Ù„ÙƒÙ„ Ù…Ø­Ø§Ø¶Ø±Ø©ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± Ø§Ù„Ù†ØµÙŠ Ø¨Ø¹Ø¯ Ø±ÙØ¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª.\n"
            "Ù…Ø«Ø§Ù„: Ø¥Ø°Ø§ Ø±ÙØ¹Øª Ù…Ø­Ø§Ø¶Ø±ØªÙŠÙ† ÙˆØªØ±ÙŠØ¯ 3 Ø£Ø³Ø¦Ù„Ø© Ù„Ù„Ø£ÙˆÙ„Ù‰ Ùˆ 7 Ù„Ù„Ø«Ø§Ù†ÙŠØ©ØŒ Ø£Ø±Ø³Ù„:\n"
            "`/generate 3 7`\n\n"
            "Ø¥Ø°Ø§ Ø£Ø±Ø¯Øª Ù†ÙØ³ Ø§Ù„Ø¹Ø¯Ø¯ (Ù…Ø«Ù„Ø§Ù‹ 4) Ù„ÙƒÙ„ Ø§Ù„Ù…Ø­Ø§Ø¶Ø±Ø§ØªØŒ Ø£Ø±Ø³Ù„:\n"
            "`/generate 4`",
            parse_mode='Markdown'
        )
        # ÙŠÙ…ÙƒÙ† ØªØ­Ø¯ÙŠØ« Ù„ÙˆØ­Ø© Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ù‡Ù†Ø§ Ø£ÙŠØ¶Ù‹Ø§ Ø¥Ø°Ø§ Ø£Ø±Ø¯Ù†Ø§
        # keyboard = build_main_keyboard(context, chat_id)
        # await query.edit_message_reply_markup(reply_markup=keyboard)


# --- Ù…Ø¹Ø§Ù„Ø¬ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª ---
async def handle_document(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """ÙŠØ¹Ø§Ù„Ø¬ Ù…Ù„ÙØ§Øª PDF Ø§Ù„Ù…Ø±Ø³Ù„Ø©."""
    chat_id = update.effective_chat.id
    document = update.message.document
    user = update.effective_user

    if document.mime_type == 'application/pdf':
        if chat_id not in context.user_data or 'pdfs' not in context.user_data[chat_id]:
            context.user_data[chat_id] = {'pdfs': []} # Ø¶Ù…Ø§Ù† Ø§Ù„ØªÙ‡ÙŠØ¦Ø©

        # Ø¥Ø¸Ù‡Ø§Ø± Ø±Ø³Ø§Ù„Ø© Ø¨Ø£Ù† Ø§Ù„Ù…Ù„Ù Ù‚ÙŠØ¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        processing_msg = await update.message.reply_text(f"â³ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù: {document.file_name}...")

        file = await document.get_file()
        temp_dir = "temp_pdfs"
        os.makedirs(temp_dir, exist_ok=True)
        file_path = os.path.join(temp_dir, f"{chat_id}_{document.file_unique_id}_{document.file_name}")

        try:
            await file.download_to_drive(custom_path=file_path)
            logger.info(f"Downloaded PDF: {file_path} from user {user.id}")
        except Exception as download_error:
            logger.error(f"Failed to download file {document.file_name} from user {user.id}: {download_error}")
            await processing_msg.edit_text(f"âŒ Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù: {document.file_name}.")
            return

        extracted_text = extract_text_from_pdf(file_path)

        final_status_message = ""
        if "Error:" in extracted_text:
            final_status_message = f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© {document.file_name}: {extracted_text}"
            logger.error(f"Text extraction error for {document.file_name} (user {user.id}): {extracted_text}")
        elif not extracted_text.strip():
             final_status_message = f"âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙŠ Ù†Øµ Ù…Ù† Ø§Ù„Ù…Ù„Ù {document.file_name}. Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø§Ù„Ù…Ù„Ù ØµÙˆØ±Ø© Ø£Ùˆ ÙØ§Ø±ØºØ§Ù‹."
             logger.warning(f"No text extracted from {document.file_name} (user {user.id}).")
        else:
            context.user_data[chat_id]['pdfs'].append({
                'filename': document.file_name,
                'text': extracted_text
            })
            pdf_count = len(context.user_data[chat_id]['pdfs'])
            final_status_message = (
                f"âœ… ØªÙ…Øª Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø¬Ø§Ø­: {document.file_name}.\n"
                f"ğŸ“š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø­Ø§Ø¶Ø±Ø§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø© Ø§Ù„Ø¢Ù†: {pdf_count}."
            )
            logger.info(f"Successfully processed {document.file_name} for user {user.id}. Total PDFs: {pdf_count}")

        # ØªØ­Ø±ÙŠØ± Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        await processing_msg.edit_text(final_status_message)

        # Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª
        if os.path.exists(file_path):
            try:
                os.remove(file_path)
                logger.info(f"Removed temporary file: {file_path}")
            except Exception as remove_error:
                logger.error(f"Failed to remove temporary file {file_path}: {remove_error}")

        # Ø¥Ø±Ø³Ø§Ù„ Ù„ÙˆØ­Ø© Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ù…Ø¬Ø¯Ø¯Ù‹Ø§ Ø¨Ø¹Ø¯ Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£Ø²Ø±Ø§Ø± (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        # keyboard = build_main_keyboard(context, chat_id)
        # await update.message.reply_text("ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† Ø¥Ø±Ø³Ø§Ù„ Ù…Ù„Ù Ø¢Ø®Ø± Ø£Ùˆ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©.", reply_markup=keyboard)

    else:
        await update.message.reply_text("âš ï¸ Ù…Ù† ÙØ¶Ù„Ùƒ Ø£Ø±Ø³Ù„ Ù…Ù„Ù PDF ÙÙ‚Ø·.")


# --- Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø£Ù…Ø± Ø§Ù„Ù†ØµÙŠ generate (Ù„Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø®ØµØµ) ---
async def generate_command_handler(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
     """ÙŠØ¹Ø§Ù„Ø¬ Ø§Ù„Ø£Ù…Ø± Ø§Ù„Ù†ØµÙŠ /generate N1 N2 ..."""
     logger.info(f"Received /generate command with args: {context.args} from user {update.effective_user.id}")
     if not context.args:
         await update.message.reply_text(
             "âš ï¸ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ù…Ø± Ø§Ù„Ù†ØµÙŠØŒ ÙŠØ±Ø¬Ù‰ ØªØ­Ø¯ÙŠØ¯ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©.\n"
            "Ù…Ø«Ø§Ù„: `/generate 5` (5 Ø£Ø³Ø¦Ù„Ø© Ù„ÙƒÙ„ Ù…Ø­Ø§Ø¶Ø±Ø©)\n"
            "Ø£Ùˆ `/generate 3 7` (3 Ù„Ù„Ù…Ø­Ø§Ø¶Ø±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ØŒ 7 Ù„Ù„Ø«Ø§Ù†ÙŠØ©).",
             parse_mode='Markdown'
         )
         return
     await _perform_generation(update, context, context.args)


# --- Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø£ÙˆØ§Ù…Ø± ØºÙŠØ± Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ© ---
async def unknown_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    await update.message.reply_text("Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ÙÙ‡Ù… Ù‡Ø°Ø§ Ø§Ù„Ø£Ù…Ø±. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø²Ø±Ø§Ø± Ø£Ùˆ Ø§Ù„Ø£Ù…Ø± `/start` Ù„Ù„Ø¨Ø¯Ø¡.")

# --- Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ---
def main() -> None:
    """ÙŠØ¨Ø¯Ø£ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª."""
    if not model:
        logger.error("Google AI Model is not initialized. Aborting bot startup.")
        print("Error: Google AI Model failed to initialize. Please check your API key and configuration.")
        return

    application = Application.builder().token(TELEGRAM_BOT_TOKEN).build()

    # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ø§Ù„Ø¬Ø§Øª Ø§Ù„Ø£ÙˆØ§Ù…Ø± ÙˆØ§Ù„Ø£Ø²Ø±Ø§Ø± ÙˆØ§Ù„Ø±Ø³Ø§Ø¦Ù„
    application.add_handler(CommandHandler("start", start))
    application.add_handler(CommandHandler("generate", generate_command_handler)) # <-- Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø£Ù…Ø± Ø§Ù„Ù†ØµÙŠ
    # Ù„Ù… Ù†Ø¹Ø¯ Ø¨Ø­Ø§Ø¬Ø© Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø£Ù…Ø± /clear Ù…Ù†ÙØµÙ„ØŒ ÙŠØªÙ… Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹Ù‡ Ø¹Ø¨Ø± Ø§Ù„Ø²Ø±
    # application.add_handler(CommandHandler("clear", clear_pdfs_command_handler))

    application.add_handler(CallbackQueryHandler(button_callback)) # <-- Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø£Ø²Ø±Ø§Ø±

    application.add_handler(MessageHandler(filters.Document.PDF, handle_document)) # <-- Ù…Ø¹Ø§Ù„Ø¬ Ù…Ù„ÙØ§Øª PDF
    application.add_handler(MessageHandler(filters.Document.ALL & (~filters.Document.PDF),
        lambda update, context: update.message.reply_text("âš ï¸ Ù…Ù† ÙØ¶Ù„Ùƒ Ø£Ø±Ø³Ù„ Ù…Ù„ÙØ§Øª PDF ÙÙ‚Ø·."))) # Ù…Ø¹Ø§Ù„Ø¬ Ù…Ù„ÙØ§Øª ØºÙŠØ± PDF

    # Ù…Ø¹Ø§Ù„Ø¬ Ù„Ù„Ø£ÙˆØ§Ù…Ø± ØºÙŠØ± Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ© (ÙŠÙØ¶Ù„ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø£Ø®ÙŠØ±)
    application.add_handler(MessageHandler(filters.COMMAND, unknown_command))


    logger.info("Bot is starting with Inline Keyboard and Google AI...")
    application.run_polling()

if __name__ == "__main__":
    # --- Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø®Ø§Øµ Ø¨Ù€ Google AI (generate_mcqs_from_text) ÙŠØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡Ùˆ ---
    async def generate_mcqs_from_text(lecture_text: str, num_questions: int, lecture_name: str) -> str:
        """ÙŠÙ†Ø´Ø¦ Ø£Ø³Ø¦Ù„Ø© MCQ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Google Gemini API."""
        if not model:
            return f"Sorry, the Google AI model client is not initialized. Cannot generate MCQs for '{lecture_name}'."

        MAX_TEXT_LENGTH = 50000
        original_length = len(lecture_text)
        if original_length > MAX_TEXT_LENGTH:
            lecture_text = lecture_text[:MAX_TEXT_LENGTH]
            warning_msg = (f"\n\n(Ù…Ù„Ø§Ø­Ø¸Ø©: ØªÙ… Ø§Ù‚ØªØµØ§Øµ Ù†Øµ Ø§Ù„Ù…Ø­Ø§Ø¶Ø±Ø© '{lecture_name}' "
                           f"Ù…Ù† {original_length} Ø¥Ù„Ù‰ {MAX_TEXT_LENGTH} Ø­Ø±ÙØ§Ù‹ "
                           f"Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©. Ø³ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ù…Ù† Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„ ÙÙ‚Ø·.)")
        else:
            warning_msg = ""

        prompt = f"""You are an expert AI assistant specializing in creating high-quality educational multiple-choice questions (MCQs).
Your task is to generate exactly {num_questions} MCQs in English based ONLY on the provided lecture content from "{lecture_name}".

**Instructions:**
1.  Read the lecture content carefully.
2.  Create {num_questions} distinct MCQs that test understanding of the key concepts in the text.
3.  Each MCQ must follow this specific format STRICTLY:
    [Question Number]. [Question Text]
    A) [Option A]
    B) [Option B]
    C) [Option C]
    D) [Option D]
    Answer: [Correct Letter]
4.  Ensure there are exactly four options (A, B, C, D) for each question.
5.  Clearly indicate the single correct answer using "Answer: [Letter]".
6.  **Crucially:** Do NOT include any introductory sentences, concluding remarks, explanations, apologies, or any text whatsoever outside of the MCQ questions themselves in the specified format. Your entire output should consist only of the numbered questions, options, and answers.

**Lecture Content:**
--- START OF CONTENT ---
{lecture_text}
--- END OF CONTENT ---

Generate the MCQs now:
"""

        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        ]

        generation_config = genai.types.GenerationConfig(
            temperature=0.5
        )

        retries = 2
        for attempt in range(retries):
            try:
                response = await asyncio.to_thread(
                    model.generate_content,
                    contents=prompt,
                    generation_config=generation_config,
                    safety_settings=safety_settings
                )

                if not response.candidates:
                     block_reason = "Unknown safety block"
                     try:
                         block_reason = response.prompt_feedback.block_reason.name
                     except Exception:
                         pass
                     error_message = f"Sorry, the request for '{lecture_name}' was blocked by safety filters (Reason: {block_reason}). This might happen with sensitive topics. Try modifying the content if possible."
                     logger.warning(f"Safety block for '{lecture_name}': {block_reason}. Prompt feedback: {response.prompt_feedback}")
                     return error_message

                mcqs = response.text.strip()

                if not mcqs or not ("Answer:" in mcqs or "A)" in mcqs):
                     logger.warning(f"Gemini response for '{lecture_name}' seems empty or invalid: '{mcqs[:100]}...'")
                     if attempt < retries - 1:
                         await asyncio.sleep(2)
                         continue
                     else:
                        return f"Sorry, the AI model returned an unexpected or empty response for '{lecture_name}' after {retries} attempts. Response snippet: '{mcqs[:100]}...'"

                # Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ø¥Ø¶Ø§ÙÙŠ Ù…Ù† Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ ÙˆÙ„ÙƒÙ†Ù‡ Ù‚Ø¯ ÙŠØ³Ø§Ø¹Ø¯)
                # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ ÙŠØ¨Ø¯Ø£ Ø¨Ø±Ù‚Ù… Ø£Ùˆ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 'Answer:'
                lines = mcqs.splitlines()
                if not lines or not (lines[0].strip().startswith(('1.', '2.', '3.')) or 'Answer:' in lines[0]):
                     logger.warning(f"Gemini response for '{lecture_name}' does not seem to follow the expected MCQ format. Snippet: '{mcqs[:150]}...'")
                     # Ù„Ø§ Ù†Ø¹ØªØ¨Ø±Ù‡ Ø®Ø·Ø£ ÙØ§Ø¯Ø­Ù‹Ø§ Ø¨Ø§Ù„Ø¶Ø±ÙˆØ±Ø©ØŒ Ù„ÙƒÙ† Ù†Ø³Ø¬Ù„ ØªØ­Ø°ÙŠØ±Ù‹Ø§

                return mcqs + warning_msg

            except Exception as e:
                logger.error(f"Google AI API error (Attempt {attempt+1}/{retries}) for lecture {lecture_name}: {e}")
                if attempt < retries - 1:
                    wait_time = (attempt + 1) * 3
                    logger.info(f"API error occurred. Retrying in {wait_time} seconds...")
                    await asyncio.sleep(wait_time)
                    continue
                else:
                    error_details = str(e)
                    user_message = f"Sorry, I couldn't generate MCQs for '{lecture_name}'. An error occurred after {retries} attempts."
                    if "API key not valid" in error_details:
                        user_message += " Please check if the Google API Key in the code is correct."
                    elif "quota" in error_details.lower():
                         user_message += " You might have exceeded the usage limits for the free tier. Please check your Google AI Studio usage."
                    elif "429" in error_details or "resource_exhausted" in error_details.lower():
                         user_message += " The service is currently busy or rate limits exceeded. Please try again in a moment."
                    else:
                         user_message += f" Error details: {error_details[:100]}..."
                    return user_message

        return f"Sorry, I couldn't generate MCQs for '{lecture_name}' after {retries} attempts due to repeated errors."
    # ---------------------------------------------------------------------
    main() # <-- Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ø¨Ø¯Ø¡ Ø§Ù„Ø¨ÙˆØª